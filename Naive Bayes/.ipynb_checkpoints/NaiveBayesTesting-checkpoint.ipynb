{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ae5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GaussianNaiveBayes as NB_gaussian\n",
    "import BernoulliNaiveBayes as NB_bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015e8ab",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "### Problem Setting: \n",
    "* A data set with n samples ${(x_1,y_1), (x_2,y_2),...,(x_3,y_3)}$\n",
    "* each $x_i$ is a feature vector: $x_1=[x_{i1}, x_{i2},...,x_{id}]$ where $d$ is the number of features\n",
    "* $y_i$ is the class label for $x_i$ and takes values from a set of classes $C$\n",
    "\n",
    "### Bayes Theorem\n",
    "The posteriao probability for a class $c$ givne a feature vector $x$ is:\n",
    "$$P(y=c|x)=\\frac{P(x|y=c)P(y=c)}{P(x)}$$\n",
    "$$posterior \\varpropto likelihood * prior$$\n",
    "where:\n",
    "* $P(y=c|x)$ is the **likelihood**, which is the probability of observing the feature vector $x$ given that this sample is of class $c$.\n",
    "* $P(y=c)$ is the **pripor**, whcih is the porbability of any sample being of class $c$ without observing it.\n",
    "* $P(x)$ is the evidence, which is probability of observing the feature vector $x$\n",
    "\n",
    "### Naive Assumption\n",
    "The \"naive\" in Naive Bayes comes from the assumption that **each feature in the dataset is independent of all other features, given the class label**. This allows us to simplify the likelihood as:\n",
    "$$P(x | y=c) = \\prod_{j=1}^{d} P(x_j | y=c)$$\n",
    "\n",
    "### Classification\n",
    "To classify a new sample $x$, we compute the posterior probability for each class c and choose the class with the highest posterior probability. The evidence $P(x)$ is the same for all classes, so it can be ignored for this purpose:\n",
    "$$\\hat{y} = \\arg\\max_{c \\in C} P(y=c) \\times \\prod_{j=1}^{d} P(x_j | y=c) \\\\= \\arg\\max_{c \\in C}\\log P(y=c) + \\sum_{j=1}^{d} \\log P(x_j | y=c)\n",
    "$$\n",
    "where $\\hat{y}$ is the predicted label for $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d39021",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "The likelihood of a feature given a class is computed using the Gaussian probability density function. Primarily used for continuous features that can be assumed to have a gaussian distribution. \n",
    "### Gaussian Assumption\n",
    "For Gaussian Naive Bayes, we assume that the continuous values associated with each class are distributed according to a Gaussian distribution. For each feature $j$ and class $c$, the likelihood of a value $x_{j}$ given the class $c$ is:\n",
    "$$P(x_j | y=c) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{jc}}} \\exp\\left(-\\frac{(x_j - \\mu_{jc})^2}{2\\sigma^2_{jc}}\\right)\n",
    "$$\n",
    "where:\n",
    "* $\\mu_{jc}$ is the mean of feature $j$ for samples from class $c$\n",
    "* $\\sigma_{jc}^{2}$ is the variance of feature $j$ for samples from class $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6205dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset and use only two classes for binary classification\n",
    "data = load_iris()\n",
    "X = data.data[data.target != 2]\n",
    "y = data.target[data.target != 2]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82d4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the Gaussian Naive Bayes classifier\n",
    "clf = NB_gaussian.GaussianNaiveBayes()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999287c",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes (with Laplace Smoothing)\n",
    "\n",
    "### Bernoulli Distribution\n",
    "The probability mass function of Bernoulli distribution is:\n",
    "$$P(X=k) = p^k (1-p)^{1-k}$$\n",
    "where $k$ can be either 0 or 1, and $p$ is the probability of success(i.e.$X=1$)\n",
    "\n",
    "### Laplace Smoothing\n",
    "Laplace smoothing in Naive Bayes prevents zero probabilities for unseen feature-class combinations, ensuring the model remains applicable to new data by assigning small non-zero probabilities to unobserved events.\n",
    "\n",
    "Laplace smoothing (or add-one smoothing) when estimating probabilities from frequency counts is:\n",
    "$$P(x) = \\frac{\\text{count}(x) + \\alpha}{N + \\alpha k}$$\n",
    "where:\n",
    "* $\\text{count}(x)$ is the number of times event x occurs in the data.\n",
    "* $N$ is the total number of events.\n",
    "* $k$ is the number of possible distinct events.\n",
    "* $\\alpha$ is the smoothing parameter\n",
    "\n",
    "With Laplace Smoothing, the smoothed prior probability $P(y_i)=c$ for a class $c$ is:\n",
    "$$P(y_i = c) = \\frac{\\text{count}(y_i = c) + \\alpha}{N + \\alpha k}$$\n",
    "\n",
    "### Bernoulli Assumption\n",
    "For Bernoulli Naive Bayes, we assume that the binary values associated with each class are distributed according to a Bernoulli distribution. For each feature $j$ and class $c$, the likelihood of a value $x_{j}$ given the class $c$ is:\n",
    "$$P(x_j | y=c) = p_{jc}^{x_j}(1-p_{jc})^{(1-{x_j})}\\\\\n",
    "=P(x_j=1|y=c)^{x_j}(1-P(x_j=1|y=c))^{(1-{x_j})}$$\n",
    "where $p_{jc}=P(x_j=1|y=c)$ is the probability of each feature being 1 (or present) given a class. For each feature $j$ and class $c$:\n",
    "$$P(x_j = 1 | y=c) = \\frac{\\sum_{i=1}^{n} I(x_{ij} = 1 \\land y_i = c) + \\alpha}{\\sum_{i=1}^{n} I(y_i = c) + 2\\alpha}\n",
    "$$\n",
    "where:\n",
    "* $I(\\cdot)$ is the indicator function that returns 1 if the condition inside is true and 0 otherwise.\n",
    "* $\\alpha$  is the Laplace smoothing parameter (typically set to 1 for one-unit smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dd8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 1],\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 1]\n",
    "])\n",
    "y_train = np.array([0, 1, 0, 1, 0, 1, 0, 1])\n",
    "X_test = np.array([\n",
    "    [1, 1, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 0]\n",
    "])\n",
    "y_test = np.array([0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc14aa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = NB_bernoulli.BernoulliNaiveBayes()\n",
    "clf.fit(X_train,y_train)\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f9117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
